# -*- coding: utf-8 -*-
"""ServiceHive_Assasment_Fraud_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19VYUfLzsVmemuuZdUP4Tq61G982PDVBY
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('fraud_detection.csv')
data.head()

df1 = data.copy()

"""Handeling Missing values

"""

df1.isnull().sum()

missing_values = df1.isnull().sum()
columns_with_missing_values = missing_values[missing_values > 0].index.tolist()
print("Columns with missing values:", columns_with_missing_values)

df1[['Amount', 'Location', 'CardHolderAge']]

df1.info()

for col in columns_with_missing_values:
  if df1[col].dtype in ['float64', 'int64']:
    print(f"Description for column: {col}")
    display(df1[col].describe())
    print("\n")
  elif df1[col].dtype == 'object':
    print(f"Value counts for column: {col}")
    display(df1[col].value_counts())
    print("\n")

df1['Location'].unique()

"""Creating Automating Pipeline

"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numeric_features = ["Amount", "CardHolderAge", "Time"]
categorical_features = ["Location", "MerchantCategory"]

numeric_transformer = Pipeline(steps=[
    ('imputer' , SimpleImputer(strategy='median')),
    ('scaler' , StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

# Use column indices instead of names for ColumnTransformer to work with numpy arrays
preprocessor = ColumnTransformer(
    transformers = [
        ('num' , numeric_transformer , [X.columns.get_loc(col) for col in numeric_features]),
        ('cat' , categorical_transformer , [X.columns.get_loc(col) for col in categorical_features])
    ]
)

"""Dropping unwanted col

"""

df1 = df1.drop(columns=['TransactionID'])

X = df1.drop(columns=["IsFraud"])
y = df1["IsFraud"]

from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2, stratify=y, random_state=42)

corr_matrix = X[numeric_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

"""Handeling imbalanced data

"""

!pip install imblearn

from imblearn.pipeline import Pipeline as ImbPipeline

preprocessing_smote_pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42))
])


X_train_resampled, y_train_resampled = preprocessing_smote_pipeline.fit_resample(X_train, y_train)

print("Shape of original training data:", X_train.shape)
print("Shape of resampled training data:", X_train_resampled.shape)
print("Distribution of the target variable in original training data:")
print(y_train.value_counts())
print("Distribution of the target variable in resampled training data:")
print(y_train_resampled.value_counts())

X_train_resampled

"""Model Training

"""

from sklearn.linear_model import LogisticRegression
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

# Create a pipeline that includes preprocessing, SMOTE, and the classifier
logisticReg_model = ImbPipeline(steps=[
    ('preprocessor' , preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier' , LogisticRegression(class_weight='balanced' , max_iter=1000))
])

# Fit the pipeline on the original training data
logisticReg_model.fit(X_train , y_train)

y_pred_lr = logisticReg_model.predict(X_test)
y_prob_lr = logisticReg_model.predict_proba(X_test)[:, 1]

"""Calculating Performance

"""

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
print("=== Logistic Regression Results ===")
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, y_prob_lr))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))

from sklearn.ensemble import RandomForestClassifier

randomForest_model = Pipeline(steps=[
    ('preprocessor' , preprocessor),
    ('classifier' , RandomForestClassifier(class_weight="balanced", random_state=42))
])

randomForest_model.fit(X_train , y_train)

y_pred_rf = randomForest_model.predict(X_test)
y_prob_rf = randomForest_model.predict_proba(X_test)[:, 1]

print("\n=== Random Forest Results ===")
print(classification_report(y_test, y_pred_rf))
print("ROC-AUC:", roc_auc_score(y_test, y_prob_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

"""Let's start by importing the necessary classification models and creating a dictionary of models to evaluate."""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier



models = {
    "Logistic Regression": LogisticRegression(class_weight='balanced', max_iter=1000),
    "Random Forest": RandomForestClassifier(class_weight="balanced", random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Support Vector Machine": SVC(probability=True, class_weight='balanced', random_state=42), # probability=True is needed for ROC-AUC
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(class_weight='balanced', random_state=42)
}

"""Define the parameter grids for each of the models."""

param_grids = {
    "Logistic Regression": {
        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]
    },
    "Random Forest": {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    },
    "Gradient Boosting": {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__learning_rate': [0.01, 0.1, 0.2]
    },
    "Support Vector Machine": {
        'classifier__C': [0.1, 1, 10],
        'classifier__gamma': ['scale', 'auto']
    },
    "K-Nearest Neighbors": {
        'classifier__n_neighbors': [3, 5, 7, 9]
    },
    "Decision Tree": {
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    }
}

from sklearn.model_selection import GridSearchCV
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

best_models = {}
grid_search_results = {}

for name, model in models.items():
    print(f"Running GridSearchCV for {name}...")


    pipeline = ImbPipeline(steps=[
        ('preprocessor' , preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier' , model)
    ])


    param_grid = param_grids[name]


    grid_search = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1
    )


    grid_search.fit(X_train, y_train)


    best_models[name] = grid_search.best_estimator_
    grid_search_results[name] = {
        "best_params": grid_search.best_params_,
        "best_score": grid_search.best_score_
    }

    print(f"Finished GridSearchCV for {name}.\n")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best cross-validation ROC-AUC: {grid_search.best_score_}\n")

"""Here are the best parameters and cross-validation ROC-AUC scores found for each model:"""

for name, result in grid_search_results.items():
    print(f"=== {name} ===")
    print(f"Best Parameters: {result['best_params']}")
    print(f"Best CV ROC-AUC: {result['best_score']:.4f}")
    print("-" * 30)

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import pandas as pd
import numpy as np

best_model_results = {}

for name, best_model in best_models.items():
    print(f"Evaluating best {name} model on the test set...")


    y_pred_best = best_model.predict(X_test)
    y_proba_best = best_model.predict_proba(X_test)[:, 1]


    best_model_results[name] = {
        "classification_report": classification_report(y_test, y_pred_best, output_dict=True),
        "roc_auc": roc_auc_score(y_test, y_proba_best),
        "confusion_matrix": confusion_matrix(y_test, y_pred_best).tolist()
    }

    print(f"Finished evaluating best {name} model.\n")


print("=== Best Model Results on Test Set ===")
for name, metrics in best_model_results.items():
    print(f"=== {name} ===")
    report_df = pd.DataFrame(metrics["classification_report"]).transpose()
    display(report_df)
    print("ROC-AUC:", metrics["roc_auc"])
    print("Confusion Matrix:\n", np.array(metrics["confusion_matrix"]))
    print("-" * 30)

"""Let's display the results for each model.

Adjusting Threshold due to less Fraud Data
"""

from sklearn.metrics import precision_recall_curve
best_model_name = max(best_model_results, key=lambda k: best_model_results[k]["roc_auc"])
best_model = best_models[best_model_name]

y_proba = best_model.predict_proba(X_test)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

target_recall = 0.8
idx = np.where(recalls >= target_recall)[0][0]
best_threshold = thresholds[idx]

y_pred_adjusted = (y_proba >= best_threshold).astype(int)

print("Best Model Selected:", best_model_name)
print("Chosen Probability Threshold:", best_threshold)
print("Classification Report with Adjusted Threshold:")
print(classification_report(y_test, y_pred_adjusted))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_adjusted))

plt.plot(recalls, precisions, marker='.')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.grid()
plt.show()

"""### Conclusion
The dataset was highly imbalanced with only about 5% fraud cases.  
I handled this by using SMOTE, class-weighted models, and hyperparameter tuning.  
I selected the model with the highest ROC-AUC and tuned the probability threshold to improve recall.  
The adjusted model detects most fraud cases while keeping false positives at a manageable level.  
For better performance in a real system, more fraud data and additional features are recommended.

"""

